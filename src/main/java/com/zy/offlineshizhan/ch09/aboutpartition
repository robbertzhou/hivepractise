关于分区：
hdfs设计的是数百万计的大文件，而非数十亿的小文件。
1、小文件过多，namenode压力过大，（150byte/文件）
2、在mapredude时，小文件会对应到很多的task。

设计策略：
1、按时间是一个好策略
2、可以设计多个分区层次
3、如果有的分区过大，则用bucket解决。